# experiment_type=spatiotemporal_action_localization_vit12

runtime:
  distribution_strategy: 'tpu'
  mixed_precision_dtype: 'bfloat16'
task:
  init_checkpoint: '/tmp/video_mae/VIT_B_16x4_MAE_PT.npy'
  init_checkpoint_modules: 'customized_vmae'
  freeze_backbone: false
  model:
    endpoint_name: 'encoded_tokens'
    backbone:
      type: 'vit_3d'
      vit_3d:
        variant: 'mae'
        pooler: 'none'
        model_name: 'vit-b16'
        temporal_patch_size: 2
        representation_size: 0
        init_stochastic_depth_rate: 0.0
        pos_embed_shape:
        - 8  # time
        - 196  # space
        transformer:
          dropout_rate: 0.0
    head:
      dropout_rate: 0.5
      num_hidden_layers: 0
      num_hidden_channels: 1024
      num_tx_channels: 768
      num_tx_heads: 6
      num_tx_layers: 1
      use_positional_embedding: true
  train_data:
    name: avakinetics
    num_examples: 353201
    zero_centering_image: false
    feature_shape: !!python/tuple
    - 16
    - 256
    - 256
    - 3
    temporal_stride: 4
    global_batch_size: 256
    dtype: 'bfloat16'
    shuffle_buffer_size: 512
    drop_remainder: true
  validation_data:
    name: avakinetics
    num_examples: 91919
    zero_centering_image: false
    feature_shape: !!python/tuple
    - 16
    - 256
    - 256
    - 3
    temporal_stride: 4
    global_batch_size: 8
    dtype: 'bfloat16'
    shuffle_buffer_size: 512
    drop_remainder: true
    import_detected_bboxes: true
trainer:
  optimizer_config:
    learning_rate:
      cosine:
        initial_learning_rate: 0.01
        decay_steps: 68984
    optimizer:
      type: 'vit_adamw'
      adamw:
        beta_1: 0.9
        beta_2: 0.999
        amsgrad: false
        weight_decay_rate: 0.00001
        exclude_from_weight_decay: !!python/list
        - 'LayerNorm'
        - 'layer_norm'
        - 'bias'
    warmup:
      linear:
        warmup_steps: 1500
  train_steps: 68984  # 50 eps
  validation_steps: 11489
  steps_per_loop: 100
  summary_interval: 100
  validation_interval: 100
  checkpoint_interval: 1000
  max_to_keep: 2
